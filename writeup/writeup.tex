%%%% ijcai18.tex

\typeout{IJCAI-18 Instructions for Authors}

% These are the instructions for authors for IJCAI-18.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).
\usepackage{ijcai18}

% Use the postscript times font!
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.


\title{Poker Project - Team 47: Modelling Poker Players}

\author{
AUYOK Sean, CAI Jiaxiu, CHIK Cheng Yao, Joyce YEO Shuhui, ZHUANG Yihui
\\ 
National University of Singapore\\
%
sean.auyok,e0201975,chikchengyao,joyceyeo,yihui@u.nus.edu
}
% If your authors do not fit in the default space, you can increase it 
% by uncommenting the following (adjust the "2.5in" size to make it fit
% properly)
% \setlength\titlebox{2.5in}

\begin{document}

\maketitle

\section{Introduction}

Artificial intelligence (AI) agents have long established their 
dominance in human games, perhaps most famously in chess with 
Deep Blue's win over Garry Kasparov in 1996. Even in games with 
imperfect information, AI agents have performed really well against 
top human experts. An interesting example would be Texas Hold'em 
Poker. In poker, a player cannot calculate all possibilities to 
arrive at a guaranteed optimal action, but also has to bluff and 
figure out whether an opponent is bluffing. More interestingly, in 
poker, it is not enough to win the opponent, but also to win by as 
large a margin as possible. This paper focuses on our team's 
development of a poker AI for Heads Up Limit Texas Hold'em played 
under strict time controls. Given the limited time for our AI agent 
to make an action, we discuss various optimisation strategies 
we adopted to improve the speed of our agent. We also discuss how we 
exploit imperfect information to maximise our gains in a stochastic 
game.

The prime functionality of our agent is that it should be rational 
and be able to recognise statistically optimal cards or combinations 
to invest in. Furthermore, our agent should be able to efficiently 
model the game state. In particular, we focus on how our agent 
abstracts card values, which is critical in processing all the 
different types of cards and combinations that could occur. A good 
abstraction allows for less calculation by our agent, which is 
important for the agent to play within the time controls.

Upon recognising strong and weak hands, our agent has to be able to 
make the right decision (whether to fold, call or bet). In order to 
give our agent the ability to make such decisions without being 
predictable, we programmed our agent with Counterfactual Regret 
Minimisation (CFR). CFR allows our agent to quickly find Nash 
Equilibria and make decisions based on a probability distribution.

However, calculation alone does not make our agent a winner. We 
trained our agent to have the ability to model and subsequently 
predict how an opponent is behaving. This enhancement allows our 
agent to gain an advantage over randomness, and maximise our gains 
from our opponent's money.

\begin{enumerate}
	\item Why did you choose this implementation (i.e. why should the reader be interested?)
	\item What is the result that you achieved? (i.e. why should the reader believe you?)
\end{enumerate}

\section{Modelling Game State}
Poker has a plethora of game states an agent can tap on, such as the hand strength, pot size, money left to name a few. Given the complexity of poker then, it would be infeasible to use all the game states in training our agent.

\subsection{Hand Strength}
The primary heuristic in a rational poker agent would be the hand strength, because that is how poker's end state is eventually evaluated. The player with the higher hand strength clears the pot. There is extensive research on evaluators hand strength, such as CactusKev, TwoPlusTwo and 7-card.

These evaluators aim to classify cards into groups of similar play styles, which are called buckets. Since hand strengths (HS) change across streets as community cards are revealed, the classification key is typically a function of the current hand strength, and the potential hand strength.

$f(n) = HS_{curr}(n) + sigma(P(HS_{new}(n)) * HS_{new}(n))$

\subsubsection{Abstracting Card Values}
To effective evaluate hand strengths, it is imperative to simplify the hands using abstraction techniques like card isomorphism. Card isomorphism relies on the fact that the type of suit a card has has no inherent value and can be abstracted out. For example, A$\clubsuit$ A$\clubsuit$ has the same winning chance as A$\spadesuit$ A$\spadesuit$. Thus, by treating these cards as the same kind of hands, the agent can act similarly for both hands.

Another way we implemented abstraction was through the use of card 
bucketing. By grouping hands with similar hand strength, like 
K$\heartsuit$ Q$\diamondsuit$ and Q$\diamondsuit$ J$\heartsuit$, the 
agent can handle hands in the same bucket with the same strategy. 
Hands can move into different buckets depending on the community 
cards that have been shown. In order to get the buckets for all 
possible hands, we used Monte Carlo simulation to estimate the 
relative strength of each hand at each betting round of the poker 
game.

We decided on five buckets as a compromise between having enough 
strategies for different types of hands and keeping the game state 
small enough for our agent to process. Percentile bucketing was 
used for easy classification, where the bottom bucket corresponds 
to the bottom 1/5 of hands according to hand strength. In order to 
provide a viable estimation of hand strength, we used the winrate of 
each hand. Each bucket is thus identified by a minimum and maximum 
winrate.

The buckets were generated for each betting round (pre-flop, flop, 
turn and river). For each round, 1000 different hands were generated 
where a hand consists of a pair of hole cards and whatever community 
cards are open for that betting round. For each of the hand, 500 
Monte Carlo simulations were generated to determine the winrate of 
the hand. The bucket cutoffs were then determined from every 20 
percentile of these 1000 hands' calculated winrate.

can probably put a table here

generate the win rate table by forcing both players to reveal their cards
OR use a mathematical approach

\section{Modelling Opponent Behaviour}

The current research on poker strategies suggests that there is no 1-king in poker tournaments. No one strategy is able to exploit all other strategies in the strategy space. The strategy graph can be described as being non-transistive, where $s_1$ dominating $s_2$ and $s_2$ dominating $s_3$ could still suggest that $s_3$ dominates $s_1$.

In that case then, an optimal agent would have to adapt to the opponent play style and find an element in the set of strategies that would dominate the opponent strategy.  There are three tasks here then, firstly to abstract the strategy states for the agent, secondly to approximate the opponent's strategy state based on the observed play and lastly to apply a dominating strategy over the opponent's play.

\subsection{Characterising Play Styles}
A simple way to characterise player strategies is to consider betting behaviour when the player has weak hands and strong hands. 

When a player receives a weak hand, they could continue playing or choose to fold. This behaviour can be defined on a tightness-looseness scale. A tight player only plays a small percentage of their hands, and folds otherwise. On the other hand, a loose player would choose to take risks and make bets based on the potential of the hand. Loose play is called bluffing, deceives the opponent into over-estimating the agent's hand strength. The opponents may fold as a result of this observation.

Theoretically, a tight play aims to reduce losses in the case of weak hands. However, a very tight play would also mean that the chances to observe the opponent's behaviour is diminished.

When a player receives a strong hand, they could call/ check (keep the pot size stable) or raise their bets. This behaviour can be defined on a passiveness-aggressiveness scale. A passive player keeps their bets low and stable. On the other hand, an aggressive player will actively make raises to increase the game stakes. Passive play is another form of deceit, which leads opponents to under-estimate the hand strength, thereby continuing to place bets and raise the pot amount.

An aggressive play style hopes to maximise the winnings when the hands are strong. However, an over-aggressive play will also encourage opponents to fold, which again discards opportunities to observe opponent behaviour.

An opponent's strategy at one point in time can then be represented as a 2-tuple of (tightness, aggressiveness). Note that this tuple merely represents an instantaneous strategy, and the opponent could change strategies over time because

\begin{enumerate}
	\item The opponent adapts their play to our agent's play style
	\item The opponent employs a team-based strategy where a coach sends out different players to the table depending on the play style our agent uses.
\end{enumerate}

\subsection{Predicting Play Styles}
Given that we decide to model player behaviour using aggressiveness and looseness, we need to extract specify features of the game state to feed into our neural network. In the given engine, the interaction with the opponent comes from the betting actions and the showdown. However, not every game ends in a showdown so using card information from the showdown may not be as effective. Hence, our heuristics for player tightness and aggressiveness is derived from the betting actions of folding and raising.

The opponent tightness can be approximated using the folding rate over multiple games. The drawback to using the folding-rate heuristic is that a large number of games is needed to make a prediction. By the time these observations are made, the opponent may have already adopted a different play style.

The opponent aggressiveness can be approximated using the raising rate. We make the assumption that when the opponent has a weak hand, they will only call or fold, and raising is only the result of relatively strong hands.

Our agent incorporates these heuristics into its modelling of the opponent by saving the opponent's action history each round.

\subsection{Deriving a Dominating Strategy}

\subsubsection{Approximating Strategy}
Given an opponent strategy, we hypothesise that approximating our strategy to be similar to the opponent would minimise our losses. We consider extreme cases to illustrate this point. Let $h_1$ denote a very strong hand and $h_2$ denote a very weak hand, $h_1$ > $h_2$.

First, consider our loose agent playing against a tight opponent. Let the opponent have $h_1$ and let our player have $h_2$. Our looseness means we will contribute an expected amount $x_1$ to the pot before folding or losing during the showdown. By pure chance, there is an equal probability that the two hands are reversed, the opponents has $h_2$ and we hold on to $h_1$. Since the opponent has a tighter play, they would have a lower probability of contributing money into the pot, and a higher probability of folding. Let $x_2$ be the opponent's expected loss. Clearly, $x_2$ < $x_1$. In the case of tightness, approximating our tightness to be close to that of the opponent will reduce our losses on weak hands.

Alternatively, consider our passive agent against an aggressive opponent.  Let the opponent have $h_1$ and let our player have $h_2$. The aggressive opponent will raise the pot amount, and eventually wins an amount $y_1$ during the showdown or when we have folded. Similarly, we have an equal chance to have $h_1$ while our opponent has $h_2$. However, our more passive play will instead opt to keep the pot size stable by calling or checking. In the end, we will win a pot amount $y_2$. Since there was less money put into the pot, then $y_2$ < $y_1$. Matching the opponent's aggressiveness, then, allows us to maximise our agent winnings.

The above analysis disregards the effects of bluffing and trapping, and the risk-taking behaviours which usually manifest with moderate-strength hands.

\subsubsection{Randomising Strategy}

The exploitability of opponent behaviour is symmetric, so our agent has to acknowledge that the opponent can similarly extract patterns from our play style. It would then be beneficial to either mask our play style or change our play style during the game.

Our agent model generates a 3-tuple for the probability of playing each action (fold, call, raise). For example, one of the tuples generated could be (0.2, 0.3, 0.5). We note here that the probability of raising is the most significant, so we can deduce that our player hand is strong. The actual magnitude of the probabilities is determined by our player aggressiveness and tightness, and the evaluated hand strength.

Some researchers employ purification techniques to overcome abstraction coarseness. These poker agents prefer the higher-probability actions and ignore actions that are unlikely to be played. In particular, Ganzfried and Sandholm found full purification to be the most effective. The full purification technique let the agent play the most-probable action with probability 1.

However, we argue against purification because noise in our player behaviour can disrupt the opponent's attempt in identifying patterns in our play style. Yakovenko et al's poker agent player against human players using a fixed play style, and after 100/ 500 hands, the human player was able to recognise mistakes made by the agent and boost their win rate. In particular, the noise supports our deceit techniques of bluffing and trapping.

Besides using noise to generate randomness, we can also change our play style to obfuscate our opponents. Given the same hand as above, our agent may instead generate a 3-tuple like (0, 0.3, 0.7), displaying a significantly higher aggressiveness. We can draw an analogy to local-hill search, where continually exploiting the same strategy may just be leading us to local maxima. The sudden exploration could be less optimal, but may also lead us to an improved local maxima. In coach-based agents, this exploration is analogous to fielding a player the opponent has not met before.

\section{Counter-factual Regret (CFR) Minimisation}

cfr
spne of counter-factual regret
regret = heuristic
shown to approximate equilibrium
\section{Training with a Deep-Q Network}

dqn


\section*{Acknowledgments}

\appendix

\section{\LaTeX{} and Word Style Files}\label{stylefiles}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai18}

\end{document}

